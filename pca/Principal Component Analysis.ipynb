{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "### An Eigenspace:\n",
    "\n",
    "<img src=\"Eigenspace.jpg\" width=\"600\" >\n",
    "\n",
    "(Image from http://alexhwoods.com/pca/)\n",
    "\n",
    "\n",
    "\n",
    "***Principal Component Analysis*** is the orthogonal linear transformation such that the greatest variance by some projection lies on the first principal component, the second greatest on the second principal component, and so on.\n",
    "\n",
    "Simplest of the true eigenvector based multivariate analyses\n",
    "\n",
    "Results of a PCA are usually discussed in terms of component scores, sometimes called factor scores (the transformed variable values corresponding to a particular data point), and loadings (the weight by which each standardized original variable should be multiplied to get the component score)\n",
    "\n",
    "Often used to visualize genetic distance and relatedness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal Transformation\n",
    "\n",
    "Is a transformation $T: V \\rightarrow V$ on a real inner product space.\n",
    "\n",
    "### What's a real inner product space?\n",
    "\n",
    "- Vector space with a structure called the inner product that is a scalar associated with every pair of vectors\n",
    "\n",
    "- Naturally induces an associated _norm_, which is a strictly positive length or size to every vector\n",
    "\n",
    "- They generalize Euclidean spaces (in which the inner product is the dot product and the norm is the magnitude) to vector spaces of any (possibly infinite) dimension\n",
    "\n",
    "### So what's an orthogonal transformation?\n",
    "\n",
    "- It's a linear transformation in a real inner product space that preserves the inner product.\n",
    "\n",
    "$\\langle u, v \\rangle \\rightarrow \\langle Tu, Tv \\rangle$\n",
    "    \n",
    "- In Euclidean space, this means preserving the dot product and therefore the angle between the two vectors as well as the magnitudes of each of the vectors.\n",
    "\n",
    "- Examples include, rotation, reflection, or any combination of the two.\n",
    "\n",
    "- Rotations have determinant 1, and reflections have determinant -1. This generalizes to higher dimensions.\n",
    "\n",
    "##### Recall Linear Transformation\n",
    "\n",
    "Given vectors $a$ and $b$,\n",
    "$T: \\mathbb{R}^{2} \\rightarrow \\mathbb{R}^{2}$ such that $T(a) + T(b) = T(a+b)$ and $T(ca) = cT(a)$\n",
    "\n",
    "$\n",
    "T = \n",
    "\\begin{bmatrix}\n",
    "    2 & -1 \\\\\n",
    "    3 & 4 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    x_{1} \\\\\n",
    "    x_{2} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    2x_{1}-x_{2} \\\\\n",
    "    3x_{1}+4x_{2} \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "(multiplication by a matrix is always a linear transformation)\n",
    "\n",
    "#### Back to Orthogonal Transformation\n",
    "\n",
    "$\n",
    "T = \n",
    "\\begin{bmatrix}\n",
    "    cos(\\theta) & -sin(\\theta) \\\\\n",
    "    sin(\\theta) & cos(\\theta) \\\\\n",
    "\\end{bmatrix}\n",
    ": \\mathbb{R}^{2} \\rightarrow \\mathbb{R}^{2}\n",
    "$\n",
    "\n",
    "The matrix representation (with respect to an orthonormal basis) of an orthogonal transformation is an orthogonal matrix.\n",
    "\n",
    "What's an orthonormal basis?\n",
    "\n",
    "A set of vectors in a vector space $V$ is a **basis** if the vectors are linearly independent and every vector in the vector space is a linear combination of this set. (for example, in Euclidean space, the unit vectors i j and k are an orthonormal basis)\n",
    "\n",
    "An **orthogonal basis** is a basis for an inner product space $V$ whose vectors are mutually orthogonal. (show difference between linearly independent and orthogonal)\n",
    "\n",
    "An **orthonormal basis** is a normalized orthogonal basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvector and Eigenvalue Intuition\n",
    "\n",
    "If you apply a linear transformation $T$ to a non-zero vector $v$ and it does not change direction, $v$ is an **eigenvector** of $T$.  Applying $T$ to the eigenvector only scales the eigenvector by the scalar value Î», called an eigenvalue. Example:\n",
    "\n",
    "<img src=\"monalisa.png\",width=400>\n",
    "\n",
    "In this linear transformation (specifically shear mapping) the blue vector is an eigenvector with eigenvalue 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Picture\n",
    "\n",
    "<img src=\"pca.png\",width=400>\n",
    "\n",
    "A super simple way to think of PCA is if you had to describe some points along a southwest-northeast line. You need coordinates x and y, but if you apply a linear transformation, you only need one axis to describe the points.\n",
    "\n",
    "In general, you want the variance along the components to describe the data as well as possible. In the image above, our number of components is equal to the n dimensions but it's useful because most of our variance is well-described along these two components.\n",
    "\n",
    "## Definition\n",
    "\n",
    "**Principal Component Analysis is the orthogonal linear transformation such that the greatest variance by some projection lies on the first principal component, the second greatest on the second principal component, and so on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of it as fitting an n-dimensional ellipsoid to the data, with each axis being a principal component. Previous example was a 2-dimensional ellipsoid. Here's some 3-dimensional ellipsoids!\n",
    "\n",
    "<img src=\"ellipsoid.png\",width=400>\n",
    "\n",
    "If an axis is small, the variance along that axis is small, and thus we only lose a bit of information by eliminating that principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for finding the components:\n",
    "\n",
    "1. Subtract the mean of each variable from the dataset to center the data around the origin\n",
    "2. Compute the covariance matrix of the data\n",
    "3. Calculate the eigenvalues and corresponding eigenvectors of the covariance matrix and normalize each of the orthogonal eigenvectors (now unit vectors)\n",
    "4. **These eigenvectors of the covariance matrix are the components in PCA.** The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues. Now you can represent data in terms of the components.\n",
    "\n",
    "*(Apparently there are two methods to calculate PCA, the correlation method and the covariance method. This is the covariance method)*\n",
    "\n",
    "Since we are taking features and ending up with components, we won't have literal meaning with respect to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Normalize the data\n",
    "\n",
    "Our $n$ x $p$ matrix of data, $X$, with column-wise zero empirical mean\n",
    "The $n$ rows represent repetitions of the experiment\n",
    "The $p$ columns give particular measurements\n",
    "\n",
    "$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "    x_{11}       & x_{12} & x_{13} & \\dots & x_{1p} \\\\\n",
    "    x_{21}       & x_{22} & x_{23} & \\dots & x_{2p} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{n1}       & x_{n2} & x_{n3} & \\dots & x_{np}\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compute covariance matrix\n",
    "\n",
    "##### Covariance Formula\n",
    "$cov_{x,y}=\\frac{\\sum_{i=1}^{N}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{N-1}$\n",
    "\n",
    "##### Covariance Matrix\n",
    "$\n",
    "C = \n",
    "\\begin{bmatrix}\n",
    "    cov(x_{1},x_{1}) & cov(x_{1},x_{2}) & cov(x_{1},x_{3}) & \\dots & cov(x_{1},x_{p}) \\\\\n",
    "    cov(x_{2},x_{1}) & cov(x_{2},x_{2}) & cov(x_{2},x_{3}) & \\dots & cov(x_{2},x_{p}) \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    cov(x_{p},x_{1}) & cov(x_{p},x_{2}) & cov(x_{p},x_{3}) & \\dots & cov(x_{p},x_{p}) \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "This matrix is symmetric $(C = C^{T})$ and the diagonal is just variances.\n",
    "\n",
    "Degenerate if you can completely represent as fewer than p components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compute eigendecomposition of covariance matrix\n",
    "\n",
    "We want the $p \\times p$ eigenvector matrix $V$ such that $V^{-1}CV = D$ where $D$ is the $p \\times p$ diagonal matrix of $C$'s eigenvalues. $V$ is the matrix of *right* eigenvectors as opposed to *left* eigenvectors, which we will see in the biplot later on.\n",
    "\n",
    "$\n",
    "D = \n",
    "\\begin{bmatrix}\n",
    "    \\lambda_{1} & 0 & \\dots & 0 \\\\\n",
    "    0 & \\lambda_{2} & \\dots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\dots & \\lambda{p} \\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now sort the eigenvectors and eigenvalues in decreasing order, rearranging D so that the pairs are maintained. Now the distribution of eigenvalues represent the \"energy content\" of each of the principal components, which is basically how important they are in describing the data. The eigenvectors/principal components form a basis for the data.\n",
    "\n",
    "If I project it there, preserves maximum variance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: You have your principal components\n",
    "\n",
    "Select the $L$ first eigenvectors such that $1 < L < p$. You are trying to choose as few as possible, maximizing the energy content of the L eigenvectors you choose.\n",
    "\n",
    "Optionally, normalize $X$ by dividing by the square roots of the diagonal of $C$. (We normalized in step 1 but apparently it's not a core part of PCA).\n",
    "\n",
    "#### Project the normalized data onto the components.\n",
    "\n",
    "$A = U \\Sigma ^{-1} V^{T} $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
